{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.2 64-bit",
      "language": "python",
      "name": "python38264bitfd92962200d74e8ba4e1089743d69310"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMXESD2cfKPb",
        "outputId": "993c83c9-fa9d-46e4-c3ea-0a529b9a4b90"
      },
      "source": [
        "!pip install gensim==4.0.0"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gensim==4.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c3/dd/5e00b6e788a9c522b48f9df10472b2017102ffa65b10bc657471e0713542/gensim-4.0.0-cp37-cp37m-manylinux1_x86_64.whl (23.9MB)\n",
            "\u001b[K     |████████████████████████████████| 23.9MB 127kB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim==4.0.0) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim==4.0.0) (1.19.5)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim==4.0.0) (5.0.0)\n",
            "Installing collected packages: gensim\n",
            "  Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed gensim-4.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VAe5vEEFsAk9",
        "outputId": "2ac95ca9-28d3-4444-d647-d81fde6d008f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIPILVFRrPcd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a69c1c0-c5d7-4f20-802e-97f2f99bb686"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as data\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import gensim\n",
        "import os\n",
        "os.chdir('/content/drive/MyDrive')\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
            "  warnings.warn(msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANZtke4MrPcq"
      },
      "source": [
        "def process_bodies(file_name):\n",
        "\n",
        "    df = pd.read_csv(file_name, header = 0)\n",
        "    dct = {row['Body ID']: row['Article Body'] for _, row in df.iterrows()}\n",
        "\n",
        "    return dct\n",
        "\n",
        "def process_stances(file_name):\n",
        "\n",
        "    df = pd.read_csv(file_name, header = 0)\n",
        "    lst = [tuple(row.tolist()) for _, row in df.iterrows()]\n",
        "\n",
        "    return lst\n",
        "\n",
        "output_mapper = {\"agree\": 0, \"disagree\": 1, \"discuss\":2, \"unrelated\": 3}\n",
        "\n",
        "def collate_fn(data, vecs = None):\n",
        "    \n",
        "    headlines = [x[0] for x in data]\n",
        "    articles = [x[1] for x in data]\n",
        "    stances = [x[2] for x in data]\n",
        "    \n",
        "    headlines_transformed = vecs.transform(headlines)\n",
        "    articles_transformed = vecs.transform(articles)\n",
        "    stances_transformed = torch.tensor([output_mapper[stance] for stance in stances])\n",
        "    \n",
        "    return headlines_transformed, articles_transformed, stances_transformed"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxEmthnorPcr"
      },
      "source": [
        "class Dataset(data.Dataset):\n",
        "    \n",
        "    def __init__(self,                               \\\n",
        "                 bodies_path = './train_bodies.csv', \\\n",
        "                 stances_path = './train_stances.csv'):\n",
        "        \n",
        "        self.bodies = process_bodies(bodies_path)\n",
        "        self.stances = process_stances(stances_path)\n",
        "    \n",
        "    def __len__(self):\n",
        "        \n",
        "        return len(self.stances)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "                 \n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        headline, body_id, stance = self.stances[idx]\n",
        "        article = self.bodies[body_id]\n",
        "\n",
        "        return headline, article, stance"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNNhSxJYrPcs"
      },
      "source": [
        "class Model(nn.Module):\n",
        "    \n",
        "    \n",
        "    def __init__(self, embedding_dim = 300,  \\\n",
        "                       n_hidden = 256,       \\\n",
        "                       pool_kernel_size = 2, \\\n",
        "                       dropout_rate = 0.5):\n",
        "        \n",
        "        super(Model, self).__init__()\n",
        "        \n",
        "        in_channels = [embedding_dim, n_hidden, n_hidden, n_hidden*2, n_hidden*2]\n",
        "        out_channels = [n_hidden, n_hidden, n_hidden*2, n_hidden*2, n_hidden*3]\n",
        "        filter_sizes = [3, 3, 3, 3, 3]\n",
        "        strides = [1, 1, 1, 1, 1]\n",
        "        paddings = [3, 3, 3, 1, 1]\n",
        "        dense_sizes = [n_hidden*3*2, n_hidden*4, n_hidden*4, n_hidden*4, 4]\n",
        "        \n",
        "        conv_layers = [nn.Conv1d(*params) for params in \\\n",
        "                       zip(in_channels, out_channels, filter_sizes, strides, paddings)]\n",
        "        pool_layers = [nn.MaxPool1d(pool_kernel_size) for _ in range(3)]\n",
        "        relu = nn.ReLU()\n",
        "        dropout = nn.Dropout(dropout_rate)\n",
        "        \n",
        "        self.cnn = nn.Sequential(conv_layers[0], relu, pool_layers[0], \\\n",
        "                                 conv_layers[1], relu, pool_layers[1], \\\n",
        "                                 conv_layers[2], relu, pool_layers[2], \\\n",
        "                                 conv_layers[3],                       \\\n",
        "                                 conv_layers[4])\n",
        "        \n",
        "        self.dnn = nn.Sequential(*[nn.Linear(in_size, out_size) \\\n",
        "                                   for in_size, out_size in     \\\n",
        "                                   zip(dense_sizes[:-1], dense_sizes[1:])])\n",
        "        \n",
        "        self.softmax = nn.Softmax(dim = -1)\n",
        "\n",
        "    def forward(self, headlines, articles):\n",
        "        \n",
        "        headlines = self.cnn(headlines)\n",
        "        headlines, _ = torch.max(headlines, axis = -1)\n",
        "        \n",
        "        articles = self.cnn(articles)\n",
        "        articles, _ = torch.max(articles, axis = -1)\n",
        "        \n",
        "        dense_vec = torch.cat((headlines, articles), axis = -1)\n",
        "        out = self.dnn(dense_vec)\n",
        "        probs = self.softmax(out)\n",
        "        \n",
        "        return probs"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYcdKu1ZrPct"
      },
      "source": [
        "def train(model, device, train_loader, criterion, optimizer, scheduler):\n",
        "    \n",
        "    # Put the model in training mode\n",
        "    model.train()\n",
        "\n",
        "    # List of train losses\n",
        "    train_loss = []\n",
        "\n",
        "    # Accuracy\n",
        "    acc = []\n",
        "    \n",
        "    for data in tqdm(train_loader):\n",
        "        \n",
        "        # Load the data, and convert the tensor with the specified device\n",
        "        headlines, articles, labels = data\n",
        "        headlines, articles, labels = headlines.to(device), articles.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        output = model(headlines, articles)\n",
        "        predictions = np.argmax(output.cpu().detach().numpy(), axis = 1)\n",
        "        acc.extend((predictions == labels.cpu().numpy()).tolist())\n",
        "\n",
        "        # Set the gradients to 0\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Compute the loss, and with it, the gradients\n",
        "        loss = criterion(output, labels)\n",
        "        train_loss.append(loss.item())\n",
        "        loss.backward()\n",
        "        \n",
        "        # Update the parameters\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate average training loss and accuracy\n",
        "    avg_train_loss = torch.mean(torch.tensor(train_loss))\n",
        "    avg_train_acc = sum(acc) / len(acc)\n",
        "\n",
        "    print(\"Training set - Average loss = {:.4f}\".format(avg_train_loss))\n",
        "    print(\"Training set - Accuracy = {:.4f}\".format(avg_train_acc))\n",
        "    time.sleep(2)\n",
        "    \n",
        "    return avg_train_loss, avg_train_acc"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDajs_uJrPcu"
      },
      "source": [
        "def test(model, device, test_loader, criterion):\n",
        "    \n",
        "    # Put the model in training mode\n",
        "    model.eval()\n",
        "\n",
        "    # List of train losses\n",
        "    test_loss = []\n",
        "\n",
        "    # Accuracy\n",
        "    acc = []\n",
        "    \n",
        "    for data in tqdm(test_loader):\n",
        "        \n",
        "        # Load the data, and convert the tensor with the specified device\n",
        "        headlines, articles, labels = data\n",
        "        headlines, articles, labels = headlines.to(device), articles.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        output = model(headlines, articles)\n",
        "        predictions = np.argmax(output.cpu().detach().numpy(), axis = 1)\n",
        "        acc.extend((predictions == labels.cpu().numpy()).tolist())\n",
        "        \n",
        "        # Compute the loss, but not the gradients\n",
        "        loss = criterion(output, labels)\n",
        "        test_loss.append(loss.item())\n",
        "\n",
        "    # Calculate average testing loss and accuracy\n",
        "    avg_test_loss = torch.mean(torch.tensor(test_loss))\n",
        "    avg_test_acc = sum(acc) / len(acc)\n",
        "\n",
        "    print(\"Testing set - Average loss = {:.4f}\".format(avg_test_loss))\n",
        "    print(\"Testing set - Accuracy = {:.4f}\".format(avg_test_acc))\n",
        "    time.sleep(2)\n",
        "    \n",
        "    return avg_test_loss, avg_test_acc"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21-JlqAQrPcv"
      },
      "source": [
        "class GoogleVec(object):\n",
        "    \n",
        "\n",
        "    def __init__(self, path = './GoogleNews-vectors-negative300.bin'):\n",
        "\n",
        "        self.model = gensim.models.KeyedVectors.load_word2vec_format(path, unicode_errors = 'ignore', binary = True)\n",
        "        self.vocab = set(self.model.index_to_key)\n",
        "        self.dct = {c: '' for c in string.punctuation.replace('.', '')}\n",
        "\n",
        "    def transform(self, X, pad = 0):\n",
        "\n",
        "        embeddings = []\n",
        "        max_len = 0\n",
        "\n",
        "        for x in X:\n",
        "\n",
        "            table = x.maketrans(self.dct)\n",
        "            x = x.translate(table)\n",
        "            v = self.vectorize(x)\n",
        "            max_len = max(max_len, v.shape[-1])\n",
        "            embeddings.append(v)\n",
        "\n",
        "        padded_embeddings = torch.full(size = (len(X), self.model.vector_size, max_len), \\\n",
        "                                       fill_value = pad,                                 \\\n",
        "                                       dtype = torch.float)\n",
        "\n",
        "        for i, v in enumerate(embeddings):\n",
        "\n",
        "            padded_embeddings[i, :, :v.shape[-1]] = v\n",
        "            \n",
        "        return padded_embeddings\n",
        "\n",
        "    def vectorize(self, text):\n",
        "    \n",
        "        words = [word for word in text.split() if word]\n",
        "        w_idx = 0\n",
        "        vectorized = []\n",
        "\n",
        "        while w_idx < len(words):\n",
        "\n",
        "            w0 = words[w_idx]\n",
        "            w1 = words[w_idx+1] if w_idx+1 < len(words) else False\n",
        "            w2 = words[w_idx+2] if w_idx+2 < len(words) else False\n",
        "\n",
        "            if w2:\n",
        "                s = '_'.join([w0, w1, w2])\n",
        "                if s in self.vocab:\n",
        "                    vectorized.append(self.model[s])\n",
        "                    w_idx += 3\n",
        "                    continue\n",
        "\n",
        "            if w1:\n",
        "                s = '_'.join([w0, w1])\n",
        "                if s in self.vocab:\n",
        "                    vectorized.append(self.model[s])\n",
        "                    w_idx += 2\n",
        "                    continue\n",
        "\n",
        "            if w0 in self.vocab:\n",
        "                vectorized.append(self.model[w0])\n",
        "            elif w0.lower() in self.vocab:\n",
        "                vectorized.append(self.model[w0.lower()])\n",
        "            else:\n",
        "                vectorized.append(self.model['</s>'])\n",
        "            w_idx += 1\n",
        "        \n",
        "        return torch.tensor(vectorized, dtype = torch.float).transpose(0, 1)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0VMWsDGrPcx"
      },
      "source": [
        "train_bodies_path = \"./train_bodies.csv\"  \n",
        "train_stances_path = \"./train_stances.csv\"\n",
        "test_bodies_path = \"./test_bodies.csv\"    \n",
        "test_stances_path = \"./test_stances.csv\"  \n",
        "embedding_dim = 300                       \n",
        "n_hidden = 256                            \n",
        "pool_kernel_size = 2                      \n",
        "dropout_rate = 0.5                       \n",
        "batch_size = 16                      \n",
        "epochs = 2                                \n",
        "learning_rate = 5e-4\n",
        "vecs = GoogleVec()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMesg1LdrPcy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1db6195-89f0-4708-c0da-5d71bc3bd964"
      },
      "source": [
        "# Importing data\n",
        "print(\"Loading datasets\")\n",
        "\n",
        "train_dataset = Dataset(bodies_path = train_bodies_path, \\\n",
        "                        stances_path = train_stances_path)\n",
        "\n",
        "test_dataset = Dataset(bodies_path = test_bodies_path, \\\n",
        "                       stances_path = test_stances_path)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading datasets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gf1DgM_UrPc0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c89a0067-7a66-4fa5-fd8e-dea5de11f0a1"
      },
      "source": [
        "# kwargs for using GPU\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print(\"Using GPU\"  if use_cuda else \"GPU not found\")\n",
        "kwargs = {\"num_workers\": 1, \"pin_memory\": True} if use_cuda else {}"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilNruPK1rPc0"
      },
      "source": [
        "train_loader = data.DataLoader(dataset = train_dataset,                           \\\n",
        "                               batch_size = batch_size,                           \\\n",
        "                               collate_fn = lambda x: collate_fn(x, vecs = vecs), \\\n",
        "                               shuffle = True)\n",
        "\n",
        "test_loader = data.DataLoader(dataset = test_dataset,                            \\\n",
        "                              batch_size = batch_size,                           \\\n",
        "                              collate_fn = lambda x: collate_fn(x, vecs = vecs), \\\n",
        "                              shuffle = False)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DghH7Fb5rPc0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "287f959a-bce3-4e75-8ed3-511d9de83b8f"
      },
      "source": [
        "# Setting up the model\n",
        "print(\"Setting up the model\")\n",
        "\n",
        "model = Model(embedding_dim = embedding_dim,       \\\n",
        "              n_hidden = n_hidden,                 \\\n",
        "              pool_kernel_size = pool_kernel_size, \\\n",
        "              dropout_rate = dropout_rate).to(device)\n",
        "\n",
        "print(\"Total model parameters =\", sum([param.nelement() for param in model.parameters()]))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting up the model\n",
            "Total model parameters = 6465796\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5i58w4NrPc1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8dc92a71-5a57-4f50-a5c3-b39f9990b033"
      },
      "source": [
        "# Optimizer, loss criterion, and learning rate scheduler\n",
        "print(\"Defining optimizer, loss criterion and learning rate scheduler\")\n",
        "\n",
        "optimizer = optim.AdamW(model.parameters(), learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "scheduler = optim.lr_scheduler.OneCycleLR(optimizer,                           \\\n",
        "                                          max_lr = learning_rate,              \\\n",
        "                                          steps_per_epoch = len(train_loader), \\\n",
        "                                          epochs = epochs,                     \\\n",
        "                                          anneal_strategy = \"linear\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Defining optimizer, loss criterion and learning rate scheduler\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQKHREdarPc1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9373b374-6203-426d-834f-c699ae2770f5"
      },
      "source": [
        "# Training the model\n",
        "print(\"Training\")\n",
        "\n",
        "train_losses, test_losses, train_acc, test_acc = [], [], [], []\n",
        "\n",
        "for epoch in range(1, epochs+1):\n",
        "\n",
        "    print(\"\\nEpoch\", epoch)\n",
        "    time.sleep(2)\n",
        "\n",
        "    avg_train_loss, avg_train_acc = train(model, device, train_loader, criterion, optimizer, scheduler)\n",
        "    train_losses.append(avg_train_loss)\n",
        "    train_acc.append(avg_train_acc)\n",
        "\n",
        "    avg_test_loss, avg_test_acc = test(model, device, test_loader, criterion)\n",
        "    test_losses.append(avg_test_loss)\n",
        "    test_acc.append(avg_test_acc)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training\n",
            "\n",
            "Epoch 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 3928/3928 [15:07<00:00,  4.33it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training set - Average loss = 1.0135\n",
            "Training set - Accuracy = 0.7329\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 784/784 [02:50<00:00,  4.61it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Testing set - Average loss = 1.0392\n",
            "Testing set - Accuracy = 0.7047\n",
            "\n",
            "Epoch 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 3928/3928 [15:08<00:00,  4.32it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training set - Average loss = 1.0108\n",
            "Training set - Accuracy = 0.7329\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 784/784 [02:48<00:00,  4.65it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Testing set - Average loss = 1.0392\n",
            "Testing set - Accuracy = 0.7047\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRXgYTNurPc1"
      },
      "source": [
        "torch.save(model.state_dict, 'model_state_dict.pt')\n",
        "torch.save(model, 'model.pt')"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBxJ3M5SUNTa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}