{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(data.Dataset):\n",
    "    \n",
    "    def __init__(self, bodies, stances):\n",
    "        \n",
    "        self.bodies = bodies\n",
    "        self.stances = stances\n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.stances)\n",
    "    \n",
    "    def collate_fn(headline, article, stance, num_sentences = 7, transformer = \"msmarco-distilbert-base-v2\"):\n",
    "\n",
    "        try:\n",
    "            encoder = SentenceTransformer(transformer)\n",
    "        except:\n",
    "            raise ValueError(\"Transformer not identified.\")\n",
    "\n",
    "        tweet_encoding = encoder.encode(headline)\n",
    "        sentences = article.split(\", \")\n",
    "        encoded_input = np.array([encoder.encode(sentence) for sentence in sentences] + tweet_encoding)\n",
    "        assert encoded_input.shape == (num_sentences + 1, 768)\n",
    "\n",
    "        output_mapper = {\"agree\": 0, \"disagree\": 1, \"discuss\":2, \"unrelated\": 3}\n",
    "        encoded_output = output_mapper[stance]\n",
    "\n",
    "        return encoded_input, encoded_output\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "                 \n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        headline, body_id, stance = self.stances.loc[idx, :]\n",
    "        article = self.bodies.loc[body_id, \"articleBody\"]\n",
    "\n",
    "        return self.collate_fn(headline = headline, article = article, stance = stance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \n",
    "    \n",
    "    def __init__(self, n_features = 8*768, \n",
    "                       n_hidden_layers = 2, \n",
    "                       hidden_layer_sizes = [1024, 64], \n",
    "                       num_classes = 4, \n",
    "                       dropout_rate = 0.1):\n",
    "        \n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        assert n_hidden_layers == len(hidden_layer_sizes)\n",
    "        \n",
    "        self.layers = [nn.Sequential(nn.Linear(n_features if n == 0 else hidden_layer_sizes[n-1], hidden_layer_sizes[n]),\n",
    "                                     nn.GELU(),\n",
    "                                     nn.Dropout(dropout_rate)) \n",
    "                       for n in range(n_hidden_layers)]\n",
    "        \n",
    "        self.output_layer = nn.Sequential(nn.Linear(hidden_layer_sizes[-1], num_classes),\n",
    "                                          nn.Softmax(dim = 0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "            \n",
    "        return self.output_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, criterion, optimizer, scheduler):\n",
    "    \n",
    "    # Put the model in training mode\n",
    "    model.train()\n",
    "\n",
    "    # List of train losses\n",
    "    train_loss = []\n",
    "    \n",
    "    for data in tqdm(train_loader):\n",
    "        \n",
    "        # Load the data, and convert the tensor with the specified device\n",
    "        encodings, labels = data\n",
    "        encodings, labels = encodings.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(encodings)\n",
    "\n",
    "        # Set the gradients to 0\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Compute the loss, and with it, the gradients\n",
    "        loss = criterion(outputs, labels)\n",
    "        train_loss.append(loss.item())\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the parameters\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate average character error rate and average word error rate\n",
    "    avg_train_loss = torch.mean(torch.tensor(train_loss))\n",
    "\n",
    "    print(\"Training set - Average loss = {:.4f}\".format(avg_train_loss))\n",
    "    time.sleep(2)\n",
    "    \n",
    "    return avg_train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader, criterion):\n",
    "    \n",
    "    # Put the model in training mode\n",
    "    model.eval()\n",
    "\n",
    "    # List of train losses\n",
    "    train_loss = []\n",
    "    \n",
    "    for data in tqdm(test_loader):\n",
    "        \n",
    "        # Load the data, and convert the tensor with the specified device\n",
    "        encodings, labels = data\n",
    "        encodings, labels = encodings.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(encodings)\n",
    "        \n",
    "        # Compute the loss, and with it, the gradients\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss.append(loss.item())\n",
    "\n",
    "    # Calculate average character error rate and average word error rate\n",
    "    avg_test_loss = torch.mean(torch.tensor(test_loss))\n",
    "\n",
    "    print(\"Training set - Average loss = {:.4f}\".format(avg_test_loss))\n",
    "    time.sleep(2)\n",
    "    \n",
    "    return avg_test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(train_bodies_path = \"./data/processed/train_bodies.csv\", \n",
    "         train_stances_path = \"./data/processed/train_stances.csv\", \n",
    "         test_bodies_path = \"./data/processed/test_bodies.csv\",\n",
    "         test_stances_path = \"./data/processed/test_stances.csv\",\n",
    "         num_sentences = 7,\n",
    "         n_hidden_layers = 2,\n",
    "         hidden_layer_sizes = [1024, 64],\n",
    "         num_classes = 4,\n",
    "         epochs = 10,\n",
    "         batch_size = 32,\n",
    "         learning_rate = 5e-4, \n",
    "         dropout_rate = 0.1):\n",
    "\n",
    "    # If current directory is not data, make one called data\n",
    "    \n",
    "    if not os.path.isdir(\"./data\"):\n",
    "        os.makedirs(\"./data\")\n",
    "\n",
    "    # Importing data\n",
    "    \n",
    "    print(\"Loading datasets\")\n",
    "    \n",
    "    train_bodies = pd.read_csv(train_bodies_path, header = 0, index_col = 0)\n",
    "    train_stances = pd.read_csv(train_stances_path, header = 0)\n",
    "    train_dataset = Dataset(train_bodies, train_stances)\n",
    "    \n",
    "    test_bodies = pd.read_csv(test_bodies_path, header = 0, index_col = 0)\n",
    "    test_stances = pd.read_csv(test_stances_path, header = 0)\n",
    "    test_dataset = Dataset(test_bodies, test_stances)  \n",
    "    \n",
    "    # kwargs for using GPU\n",
    "    \n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    print(\"Using GPU\"  if use_cuda else \"GPU not found\")\n",
    "    kwargs = {\"num_workers\": 1, \"pin_memory\": True} if use_cuda else {}\n",
    "    \n",
    "    # Loading train data\n",
    "    \n",
    "    print(\"Loading train data\")\n",
    "    \n",
    "    train_loader = data.DataLoader(dataset = train_dataset,\n",
    "                                   batch_size = batch_size,\n",
    "                                   shuffle = True)\n",
    "    \n",
    "    # Loading test data\n",
    "    \n",
    "    print(\"Loading test data\")\n",
    "    \n",
    "    test_loader = data.DataLoader(dataset = test_dataset,\n",
    "                                  batch_size = batch_size,\n",
    "                                  shuffle = False)\n",
    "\n",
    "    # Setting up the model\n",
    "    \n",
    "    print(\"Setting up the model\")\n",
    "    \n",
    "    model = Model(n_features = (num_sentences+1)*768, \n",
    "                  n_hidden_layers = n_hidden_layers, \n",
    "                  hidden_layer_sizes = hidden_layer_sizes, \n",
    "                  num_classes = num_classes, \n",
    "                  dropout_rate = dropout_rate).to(device)\n",
    "\n",
    "    print(\"Total model parameters =\", sum([param.nelement() for param in model.parameters()]))\n",
    "\n",
    "    # Optimizer, loss criterion, and learning rate scheduler\n",
    "    \n",
    "    print(\"Defining optimizer, loss criterion and learning rate scheduler\")\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters(), learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, \n",
    "                                              max_lr = learning_rate, \n",
    "                                              steps_per_epoch = len(train_loader),\n",
    "                                              epochs = epochs,\n",
    "                                              anneal_strategy = \"linear\")\n",
    "    \n",
    "    # Training the model\n",
    "    \n",
    "    print(\"\\nTraining\")\n",
    "    \n",
    "    train_losses, test_losses = [], []\n",
    "    \n",
    "    for epoch in range(1, epochs+1):\n",
    "        \n",
    "        print(\"\\nEpoch\", epoch)\n",
    "        \n",
    "        time.sleep(2)\n",
    "        train_losses.append(train(model, device, train_loader, criterion, optimizer, scheduler))\n",
    "        test_losses.append(test(model, device, test_loader, criterion))\n",
    "\n",
    "    return model, train_losses, test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets\n",
      "GPU not found\n",
      "Loading train data\n",
      "Loading test data\n",
      "Setting up the model\n",
      "Total model parameters = 260\n",
      "Defining optimizer, loss criterion and learning rate scheduler\n",
      "\n",
      "Training\n",
      "\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1691 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "collate_fn() got multiple values for argument 'headline'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-95d807be5626>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_losses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_losses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-40-ed739b1c601e>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(train_bodies_path, train_stances_path, test_bodies_path, test_stances_path, num_sentences, n_hidden_layers, hidden_layer_sizes, num_classes, epochs, batch_size, learning_rate, dropout_rate)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m         \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m         \u001b[0mtrain_losses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m         \u001b[0mtest_losses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-38-f3cf6fc52cc5>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, device, train_loader, criterion, optimizer, scheduler)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;31m# Load the data, and convert the tensor with the specified device\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tqdm\\std.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1128\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1129\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1130\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m                 \u001b[1;31m# Update and possibly print the progressbar.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    433\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 435\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    473\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    474\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 475\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    476\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    477\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-42-c95b2b7a6939>\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0marticle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbodies\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbody_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"articleBody\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mheadline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mheadline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marticle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marticle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstance\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: collate_fn() got multiple values for argument 'headline'"
     ]
    }
   ],
   "source": [
    "model, train_losses, test_losses = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python38264bitfd92962200d74e8ba4e1089743d69310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
