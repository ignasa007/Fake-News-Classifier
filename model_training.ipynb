{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cMXESD2cfKPb",
    "outputId": "993c83c9-fa9d-46e4-c3ea-0a529b9a4b90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim==4.0.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c3/dd/5e00b6e788a9c522b48f9df10472b2017102ffa65b10bc657471e0713542/gensim-4.0.0-cp37-cp37m-manylinux1_x86_64.whl (23.9MB)\n",
      "\u001b[K     |████████████████████████████████| 23.9MB 127kB/s \n",
      "\u001b[?25hRequirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim==4.0.0) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim==4.0.0) (1.19.5)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim==4.0.0) (5.0.0)\n",
      "Installing collected packages: gensim\n",
      "  Found existing installation: gensim 3.6.0\n",
      "    Uninstalling gensim-3.6.0:\n",
      "      Successfully uninstalled gensim-3.6.0\n",
      "Successfully installed gensim-4.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim==4.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QIPILVFRrPcd",
    "outputId": "8a69c1c0-c5d7-4f20-802e-97f2f99bb686"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import string\n",
    "import time\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Third party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import gensim\n",
    "from tqdm import tqdm\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "# Local application imports\n",
    "from vectors import GoogleVec\n",
    "from model_utils import *\n",
    "from model import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "LxEmthnorPcr"
   },
   "outputs": [],
   "source": [
    "class Dataset(data.Dataset):\n",
    "    \n",
    "    def __init__(self,                               \\\n",
    "                 bodies_path = './data/train_bodies.csv', \\\n",
    "                 stances_path = './data/train_stances.csv'):\n",
    "        \n",
    "        self.bodies = process_bodies(bodies_path)\n",
    "        self.stances = process_stances(stances_path)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.stances)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "                 \n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        headline, body_id, stance = self.stances[idx]\n",
    "        article = self.bodies[body_id]\n",
    "\n",
    "        return headline, article, stance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "DYcdKu1ZrPct"
   },
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, criterion, optimizer, scheduler):\n",
    "    \n",
    "    # Put the model in training mode\n",
    "    model.train()\n",
    "\n",
    "    # List of train losses\n",
    "    train_loss = []\n",
    "\n",
    "    # Accuracy\n",
    "    acc = []\n",
    "    \n",
    "    for data in tqdm(train_loader):\n",
    "        \n",
    "        # Load the data, and convert the tensor with the specified device\n",
    "        headlines, articles, labels = data\n",
    "        headlines, articles, labels = headlines.to(device), articles.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(headlines, articles)\n",
    "        predictions = np.argmax(output.cpu().detach().numpy(), axis = 1)\n",
    "        acc.extend((predictions == labels.cpu().numpy()).tolist())\n",
    "\n",
    "        # Set the gradients to 0\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Compute the loss, and with it, the gradients\n",
    "        loss = criterion(output, labels)\n",
    "        train_loss.append(loss.item())\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the parameters\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate average training loss and accuracy\n",
    "    avg_train_loss = torch.mean(torch.tensor(train_loss))\n",
    "    avg_train_acc = sum(acc) / len(acc)\n",
    "\n",
    "    print('Training set - Average loss = {:.4f}'.format(avg_train_loss))\n",
    "    print('Training set - Accuracy = {:.4f}'.format(avg_train_acc))\n",
    "    time.sleep(2)\n",
    "    \n",
    "    return avg_train_loss, avg_train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "SDajs_uJrPcu"
   },
   "outputs": [],
   "source": [
    "def test(model, device, test_loader, criterion):\n",
    "    \n",
    "    # Put the model in training mode\n",
    "    model.eval()\n",
    "\n",
    "    # List of train losses\n",
    "    test_loss = []\n",
    "\n",
    "    # Accuracy\n",
    "    acc = []\n",
    "    \n",
    "    for data in tqdm(test_loader):\n",
    "        \n",
    "        # Load the data, and convert the tensor with the specified device\n",
    "        headlines, articles, labels = data\n",
    "        headlines, articles, labels = headlines.to(device), articles.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(headlines, articles)\n",
    "        predictions = np.argmax(output.cpu().detach().numpy(), axis = 1)\n",
    "        acc.extend((predictions == labels.cpu().numpy()).tolist())\n",
    "        \n",
    "        # Compute the loss, but not the gradients\n",
    "        loss = criterion(output, labels)\n",
    "        test_loss.append(loss.item())\n",
    "\n",
    "    # Calculate average testing loss and accuracy\n",
    "    avg_test_loss = torch.mean(torch.tensor(test_loss))\n",
    "    avg_test_acc = sum(acc) / len(acc)\n",
    "\n",
    "    print('Testing set - Average loss = {:.4f}'.format(avg_test_loss))\n",
    "    print('Testing set - Accuracy = {:.4f}'.format(avg_test_acc))\n",
    "    time.sleep(2)\n",
    "    \n",
    "    return avg_test_loss, avg_test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "n0VMWsDGrPcx"
   },
   "outputs": [],
   "source": [
    "train_bodies_path = './data/train_bodies.csv'  \n",
    "train_stances_path = './data/train_stances.csv'\n",
    "test_bodies_path = './data/test_bodies.csv'    \n",
    "test_stances_path = './data/test_stances.csv'  \n",
    "embedding_dim = 300                       \n",
    "n_hidden = 256                            \n",
    "pool_kernel_size = 2                      \n",
    "dropout_rate = 0.5                       \n",
    "batch_size = 16                      \n",
    "epochs = 2                                \n",
    "learning_rate = 5e-4\n",
    "vecs = GoogleVec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NMesg1LdrPcy",
    "outputId": "f1db6195-89f0-4708-c0da-5d71bc3bd964"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets\n"
     ]
    }
   ],
   "source": [
    "# Importing data\n",
    "\n",
    "print('Loading datasets')\n",
    "\n",
    "train_dataset = Dataset(bodies_path = train_bodies_path, \\\n",
    "                        stances_path = train_stances_path)\n",
    "\n",
    "test_dataset = Dataset(bodies_path = test_bodies_path, \\\n",
    "                       stances_path = test_stances_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gf1DgM_UrPc0",
    "outputId": "c89a0067-7a66-4fa5-fd8e-dea5de11f0a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n"
     ]
    }
   ],
   "source": [
    "# kwargs for using GPU\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "print('Using GPU'  if use_cuda else 'GPU not found')\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ilNruPK1rPc0"
   },
   "outputs": [],
   "source": [
    "train_loader = data.DataLoader(dataset = train_dataset,                           \\\n",
    "                               batch_size = batch_size,                           \\\n",
    "                               collate_fn = lambda x: collate_fn(x, vecs = vecs), \\\n",
    "                               shuffle = True)\n",
    "\n",
    "test_loader = data.DataLoader(dataset = test_dataset,                            \\\n",
    "                              batch_size = batch_size,                           \\\n",
    "                              collate_fn = lambda x: collate_fn(x, vecs = vecs), \\\n",
    "                              shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DghH7Fb5rPc0",
    "outputId": "287f959a-bce3-4e75-8ed3-511d9de83b8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up the model\n",
      "Total model parameters = 6465796\n"
     ]
    }
   ],
   "source": [
    "# Setting up the model\n",
    "\n",
    "print('Setting up the model')\n",
    "\n",
    "model = Model(embedding_dim = embedding_dim,       \\\n",
    "              n_hidden = n_hidden,                 \\\n",
    "              pool_kernel_size = pool_kernel_size, \\\n",
    "              dropout_rate = dropout_rate).to(device)\n",
    "\n",
    "print('Total model parameters =', sum([param.nelement() for param in model.parameters()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H5i58w4NrPc1",
    "outputId": "8dc92a71-5a57-4f50-a5c3-b39f9990b033"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining optimizer, loss criterion and learning rate scheduler\n"
     ]
    }
   ],
   "source": [
    "# Optimizer, loss criterion, and learning rate scheduler\n",
    "\n",
    "print('Defining optimizer, loss criterion and learning rate scheduler')\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(optimizer,                           \\\n",
    "                                          max_lr = learning_rate,              \\\n",
    "                                          steps_per_epoch = len(train_loader), \\\n",
    "                                          epochs = epochs,                     \\\n",
    "                                          anneal_strategy = 'linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aQKHREdarPc1",
    "outputId": "9373b374-6203-426d-834f-c699ae2770f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3928/3928 [15:07<00:00,  4.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set - Average loss = 1.0135\n",
      "Training set - Accuracy = 0.7329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 784/784 [02:50<00:00,  4.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing set - Average loss = 1.0392\n",
      "Testing set - Accuracy = 0.7047\n",
      "\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3928/3928 [15:08<00:00,  4.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set - Average loss = 1.0108\n",
      "Training set - Accuracy = 0.7329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 784/784 [02:48<00:00,  4.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing set - Average loss = 1.0392\n",
      "Testing set - Accuracy = 0.7047\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "\n",
    "print('Training')\n",
    "\n",
    "train_losses, test_losses, train_acc, test_acc = [], [], [], []\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "\n",
    "    print('\\nEpoch', epoch)\n",
    "    time.sleep(2)\n",
    "\n",
    "    avg_train_loss, avg_train_acc = train(model, device, train_loader, criterion, optimizer, scheduler)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    train_acc.append(avg_train_acc)\n",
    "\n",
    "    avg_test_loss, avg_test_acc = test(model, device, test_loader, criterion)\n",
    "    test_losses.append(avg_test_loss)\n",
    "    test_acc.append(avg_test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "IRXgYTNurPc1"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('model_states'):\n",
    "    os.makedirs('model_states')\n",
    "    \n",
    "torch.save(model.state_dict(), './model_states/state_dict.pt')\n",
    "torch.save(model, './model_states/model.pt')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python38264bitfd92962200d74e8ba4e1089743d69310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}